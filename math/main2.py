import requests
import json
import logging
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from datetime import datetime
from tqdm import tqdm
import tiktoken
from openai import OpenAI
import tiktoken
import time
from typing import Dict, List, Optional
from tenacity import retry, wait_random_exponential, stop_after_attempt
import os
from llm_inf import LLMHandler_inf
import glob

logging.basicConfig(filename='main_testt.log', level=logging.DEBUG, encoding='utf-8', format='%(asctime)s - %(levelname)s - %(message)s')


def num_tokens_from_messages(messages, model="gpt-4.1"):
    """è®¡ç®—æ¶ˆæ¯æ€» token æ•°ï¼ˆé€‚é… OpenAI chat æ ¼å¼ï¼‰"""
    try:
        encoding = tiktoken.encoding_for_model(model)
    except KeyError:
        encoding = tiktoken.get_encoding("cl100k_base")

    # ä»¥ä¸‹æ˜¯ä¼°ç®—è§„åˆ™ï¼ˆåŸºäº OpenAI å®˜æ–¹æè¿°ï¼‰
    num_tokens = 0
    for message in messages:
        num_tokens += 4  # æ¯æ¡ message åŒ…æ‹¬ role å’Œ metadata
        for key, value in message.items():
            num_tokens += len(encoding.encode(value))
    num_tokens += 2  # åŒ…æ‹¬ assistant å›å¤å¼€å¤´
    return num_tokens

def call_api(system_prompt, prompt, api_key, model, max_tokens=2048, max_retries=10):
    url = "https://www.dmxapi.com/v1/chat/completions"
    headers = {
        'Accept': 'application/json',
        'Authorization': f'Bearer {api_key}',
        'User-Agent': 'DMXAPI/1.0.0 (https://www.dmxapi.com)',
        'Content-Type': 'application/json'
    }

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": prompt}
    ]

    is_qwen = "qwen" in model.lower()

    input_tokens = 0
    if not is_qwen:
        try:
            input_tokens = num_tokens_from_messages(messages, model)
            logging.info(f"ğŸ”¢ è¾“å…¥ Token æ•°: {input_tokens}")
        except Exception as e:
            logging.warning(f"è¾“å…¥ Token è®¡ç®—å¤±è´¥: {e}")

    payload = json.dumps({
        "model": model,
        "messages": messages,
        "temperature": 1.0
    })

    for attempt in range(max_retries):
        try:
            response = requests.post(url, headers=headers, data=payload, timeout=240)
            response.raise_for_status()
            response_json = response.json()

            if 'choices' in response_json and response_json['choices']:
                content = response_json['choices'][0]['message']['content']
                logging.info("æ¨¡å‹è¿”å›ç»“æœï¼š")
                logging.info(content)

                output_tokens = 0
                if not is_qwen:
                    try:
                        output_tokens = len(tiktoken.encoding_for_model(model).encode(content))
                        logging.info(f"ğŸ”¢ è¾“å‡º Token æ•°: {output_tokens}")
                        logging.info(f"ğŸ§® æ€» Token æ•°: {input_tokens + output_tokens}")
                    except Exception as e:
                        logging.warning(f"è¾“å‡º Token è®¡ç®—å¤±è´¥: {e}")

                return {
                    "content": content,
                    "input_tokens": input_tokens if not is_qwen else 0,
                    "output_tokens": output_tokens if not is_qwen else 0
                }
            else:
                logging.warning("API å“åº”ä¸­æœªåŒ…å« 'choices'")

        except requests.exceptions.RequestException as e:
            logging.error(f"APIè¯·æ±‚å¼‚å¸¸ (å°è¯• {attempt + 1}/{max_retries}): {e}\nè¯·æ±‚payload: {payload}")
        except (json.JSONDecodeError, KeyError) as e:
            logging.error(f"å“åº”è§£æå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
        if attempt < max_retries - 1:
            time.sleep(2)

    return {
        "content": None,
        "input_tokens": 0 if is_qwen else input_tokens,
        "output_tokens": 0
    }

def process_question(q, system_prompt, error_dir, api_key, model):
    index = q.get('index', 0)
    user_prompt = f"""
    ä¸‹é¢çš„ã€æ•°å­¦é—®é¢˜ã€‘ç”Ÿæˆç›¸åº”çš„å­¦ç”Ÿé”™è¯¯JSONæ•°æ®:
    {json.dumps(q, ensure_ascii=False, indent=2)}
    """
    handler = LLMHandler_inf(api_key, model)
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    result_content = handler.get_completion(messages, temperature=1.0, max_tokens=9999)
    result = {"content": result_content, "input_tokens": 0, "output_tokens": 0}  # tokenç»Ÿè®¡å¯é€‰
    if result["content"]:
        try:
            result_cleaned = result["content"].strip()
            if result_cleaned.startswith("```json"):
                result_cleaned = result_cleaned[7:].lstrip()
            if result_cleaned.endswith("```"):
                result_cleaned = result_cleaned[:-3].rstrip()
            output_path = error_dir / f"question_{index}_error.json"
            with open(output_path, "w", encoding="utf-8") as f:
                f.write(result_cleaned)
            return f"âœ… å†™å…¥æˆåŠŸï¼šquestion_{index}_error.json"
        except Exception as e:
            return f"âŒ å†™å…¥å¤±è´¥ï¼šquestion_{index}_error.json\né”™è¯¯ï¼š{e}\nè¾“å‡ºï¼š\n{result}"
    else:
        return f"âŒ APIè¿”å›ä¸ºç©ºï¼šindex={index}"

def generate_error():
    test_path = Path(__file__).parent / 'example_questions_100.json'
    with open(test_path, 'r', encoding='utf-8') as f:
        test_questions = json.load(f)

    error_dir = Path(__file__).parent / 'example_error_math_100'
    error_dir.mkdir(exist_ok=True)


    api_key = "sk-gOuCvUoxCi7qZ5ORL8aZkKRQDe49gi3t2rDjjl4OiViCi9VW"  
    model = "gpt-4.1"
    system_prompt = """
    **è§’è‰²:** ä½ æ˜¯ä¸€ä½ç²¾é€šæ•°å­¦æ•™è‚²å’Œå­¦ç”Ÿå­¦ä¹ å¿ƒç†å­¦çš„ä¸“å®¶ã€‚ä½ æ·±åˆ»ç†è§£å­¦ç”Ÿåœ¨è§£å†³æ•°å­¦é—®é¢˜æ—¶å¯èƒ½çŠ¯çš„å„ç§é”™è¯¯ï¼Œå¹¶èƒ½å‡†ç¡®åœ°å°†è¿™äº›é”™è¯¯ä¸ç‰¹å®šçš„è®¤çŸ¥ç»´åº¦ï¼ˆå¦‚çŸ¥è¯†æ€§ã€é€»è¾‘æ€§ã€ç­–ç•¥æ€§ã€æ‰§è¡Œæ€§ï¼‰è”ç³»èµ·æ¥ã€‚

    **ä»»åŠ¡:** æˆ‘å°†ä¸ºä½ æä¾›ä¸€ä¸ªã€é”™è¯¯ç»´åº¦è¡¨æ ¼ã€‘å’Œä¸€ä¸ªå…·ä½“çš„ã€æ•°å­¦é—®é¢˜ã€‘ã€‚è¯·ä½ ä¸¥æ ¼éµå¾ªä»¥ä¸‹è¦æ±‚ï¼Œä¸ºè¯¥é—®é¢˜**å›ºå®šç”Ÿæˆå…­ç§ä¸åŒç±»å‹çš„å­¦ç”Ÿé”™è¯¯**JSONæ•°æ®ã€‚è¿™å…­ç§é”™è¯¯å¿…é¡»ä¸¥æ ¼æŒ‰ç…§**å››ç§å•ç»´åº¦é”™è¯¯**å’Œ**ä¸¤ç§å¤šç»´åº¦å¤åˆé”™è¯¯**çš„ç»“æ„è¿›è¡Œç»„ç»‡ã€‚

    **æ ¸å¿ƒè¦æ±‚:**

    1.  **å›ºå®šçš„é”™è¯¯ç»“æ„:** ä½ å¿…é¡»ä¸å¤šä¸å°‘ï¼Œæ­£å¥½ç”Ÿæˆå…­ä¸ªé”™è¯¯JSONå¯¹è±¡ã€‚å…¶ç»“æ„å¿…é¡»å¦‚ä¸‹ï¼š
        * å‰å››ä¸ªé”™è¯¯å¯¹è±¡å¿…é¡»æ˜¯**å•ç»´åº¦é”™è¯¯**ï¼Œä¾æ¬¡åˆ†åˆ«å¯¹åº”ã€çŸ¥è¯†æ€§ã€‘ã€ã€é€»è¾‘æ€§ã€‘ã€ã€ç­–ç•¥æ€§ã€‘å’Œã€æ‰§è¡Œæ€§ã€‘è¿™å››ä¸ªç»´åº¦ã€‚æ¯ä¸ªé”™è¯¯éƒ½åº”æ˜¯è¯¥ç»´åº¦çš„å…¸å‹ã€çº¯ç²¹çš„ä½“ç°ã€‚
        * åä¸¤ä¸ªé”™è¯¯å¯¹è±¡å¿…é¡»æ˜¯**å¤šç»´åº¦ï¼ˆæ·±åº¦å¤æ‚ï¼‰é”™è¯¯**ï¼Œæ¯ä¸ªé”™è¯¯å¿…é¡»ç”±è‡³å°‘ä¸¤ä¸ªç»´åº¦å…±åŒå¯¼è‡´ï¼Œç”¨ä»¥æ¨¡æ‹ŸçœŸå®ä¸–ç•Œä¸­å­¦ç”Ÿé”™è¯¯æˆå› çš„å¤æ‚æ€§ã€‚

    2.  **æ¨¡æ‹ŸçœŸå®æ€§:** ç”Ÿæˆçš„æ¯ä¸ªé”™è¯¯éƒ½å¿…é¡»é«˜åº¦æ¨¡æ‹ŸçœŸå®å­¦ç”Ÿå¯èƒ½å‡ºç°çš„æ€ç»´å’Œæ“ä½œè¿‡ç¨‹ã€‚åŸå› è§£é‡Šè¦åˆç†ã€æœ‰è¯´æœåŠ›ï¼Œä¸èƒ½ç”Ÿæ¬ç¡¬å¥—è¡¨æ ¼ä¸­çš„å®šä¹‰ã€‚

    3.  **å¤šç»´åº¦å½’å› :** å¯¹äºé‚£ä¸¤ä¸ª**å¤šç»´åº¦å¤åˆé”™è¯¯**ï¼Œä½ çš„åˆ†æå¿…é¡»æ¸…æ™°åœ°ä½“ç°å‡ºå¤šä¸ªâ€œé”™è¯¯ç»´åº¦â€æ˜¯å¦‚ä½•å…±åŒä½œç”¨ï¼Œå¯¼è‡´æœ€ç»ˆçš„é”™è¯¯è¡¨ç°çš„ã€‚

    4.  **ç²¾ç¡®åˆ°ç‚¹:** "å…·ä½“é”™è¯¯ç‚¹"å¿…é¡»æ˜ç¡®æŒ‡å‡ºåœ¨è§£å†³è¿™ä¸ªç‰¹å®šã€æ•°å­¦é—®é¢˜ã€‘æ—¶ï¼Œå­¦ç”Ÿåœ¨å“ªä¸€æ­¥ã€å“ªä¸ªå…¬å¼ã€å“ªä¸ªæ•°å­—ä¸Šå‡ºé”™äº†ã€‚

    5.  **æ·±åº¦åˆ†æ:** "é”™è¯¯ç»´åº¦å¯¼è‡´é”™è¯¯ç‚¹å‡ºç°åŸå› "æ˜¯æ ¸å¿ƒï¼Œéœ€è¦è¯¦ç»†ã€æ¸…æ™°åœ°è§£é‡Šä¸ºä»€ä¹ˆå­¦ç”Ÿä¼šå› ä¸ºè¿™ä¸ª/è¿™äº›ç»´åº¦ä¸Šçš„ç¼ºé™·ï¼Œæœ€ç»ˆå¯¼è‡´äº†é‚£ä¸ªå…·ä½“çš„é”™è¯¯ã€‚

    6.  **é‡ç‚¹æ ‡æ³¨:** åœ¨ç”Ÿæˆçš„**å…­ä¸ª**é”™è¯¯ä¸­ï¼Œè¯·æ ¹æ®ä½ çš„ä¸“ä¸šåˆ¤æ–­ï¼Œé€‰æ‹©å‡º**ä¸€ä¸ª**çœŸå®å­¦ç”Ÿæœ€å®¹æ˜“çŠ¯çš„é”™è¯¯ï¼Œå¹¶å°†å…¶`main`å­—æ®µè®¾ç½®ä¸º1ã€‚å…¶ä½™äº”ä¸ªçš„`main`å­—æ®µå¿…é¡»ä¸º0ã€‚

    7.  **ä¸¥æ ¼çš„JSONæ ¼å¼:** è¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªåŒ…å«**å…­ä¸ª**å¯¹è±¡çš„ã€æ ‡å‡†çš„ã€æ— æ³¨é‡Šçš„JSONæ•°ç»„ã€‚ä¸è¦æ·»åŠ æ–°çš„keyã€‚"index"ä¸ºä»0å¼€å§‹çš„æ•°å€¼ç¼–å·ã€‚

    8.  **ä¿è¯é”™è¯¯:** "å…·ä½“é”™è¯¯ç‚¹"ä¸­å¿…é¡»æ˜ç¡®æŒ‡å‡ºå­¦ç”Ÿå¾—åˆ°çš„é”™è¯¯ç­”æ¡ˆæˆ–è€…æ˜¯æ— æ³•ç»™å‡ºå…·ä½“ç­”æ¡ˆï¼Œé”™è¯¯ç­”æ¡ˆå¿…é¡»ä¸æ­£ç¡®ç­”æ¡ˆä¸åŒã€‚

    ---

    **ã€é”™è¯¯ç»´åº¦è¡¨æ ¼ã€‘**

    | é”™è¯¯ç»´åº¦ (Error Dimension) | æ ¸å¿ƒå®šä¹‰ (Core Definition)                                 | å…¸å‹è¡¨ç° (Typical Manifestations)                        |
    | -------------------------- | ---------------------------------------------------------- | -------------------------------------------------------- |
    | **çŸ¥è¯†æ€§é”™è¯¯ (Knowledge-Based)** | å› æ•°å­¦çŸ¥è¯†æœ¬èº«çš„ç¼ºé™·ã€é—å¿˜æˆ–ç†è§£ä¸æ·±åˆ»å¯¼è‡´çš„é”™è¯¯ã€‚         | æ¦‚å¿µæ··æ·†ã€æ€§è´¨è¯¯è®°ã€å…¬å¼é”™ç”¨ã€å‰ææ¡ä»¶å¿½è§†ã€‚             |
    | **é€»è¾‘æ€§é”™è¯¯ (Logical)** | å› è¿èƒŒæ•°å­¦çš„ä¸¥è°¨æ€§ã€æ¨ç†è§„åˆ™å’Œé€»è¾‘å½¢å¼è€Œäº§ç”Ÿçš„é”™è¯¯ã€‚         | å¾ªç¯è®ºè¯ã€åˆ†ç±»è®¨è®ºä¸å®Œå¤‡ã€å·æ¢æ¦‚å¿µã€ä¸ç­‰ä»·å˜æ¢ã€‚         |
    | **ç­–ç•¥æ€§é”™è¯¯ (Strategic)** | å› æœªèƒ½é€‰æ‹©æˆ–æ‰§è¡Œæœ‰æ•ˆçš„è§£é¢˜è·¯å¾„å’Œæ€æƒ³æ–¹æ³•è€Œå¯¼è‡´çš„é”™è¯¯ã€‚     | æ¨¡å¼è¯†åˆ«å¤±è´¥ã€ç¼ºä¹æ•´ä½“è§‚ã€æ€ç»´åƒµåŒ–ã€æ— æ³•è½¬åŒ–é—®é¢˜ã€‚       |
    | **æ‰§è¡Œæ€§é”™è¯¯ (Execution)** | åœ¨è§£é¢˜çš„å…·ä½“æ“ä½œç¯èŠ‚å‡ºç°çš„å¤±è¯¯ï¼Œå¸¸è¢«ç¬¼ç»Ÿåœ°ç§°ä¸º"ç²—å¿ƒ"ã€‚     | å®¡é¢˜ä¸æ¸…ã€æŠ„å†™é”™è¯¯ã€è®¡ç®—å¤±è¯¯ã€ä¹¦å†™ä¸è§„èŒƒã€‚               |

    ---

    **ã€JSON è¾“å‡ºæ ¼å¼ã€‘**

    ```json
    [
        {
            "é”™è¯¯ç»´åº¦": ["çŸ¥è¯†æ€§"],
            "å…¸å‹è¡¨ç°": ["è¡¨ç°A"],
            "å…·ä½“é”™è¯¯ç‚¹": "...",
            "é”™è¯¯ç»´åº¦å¯¼è‡´é”™è¯¯ç‚¹å‡ºç°åŸå› ": "...",
            "main": 0,
            "index": 0
        },
        {
            "é”™è¯¯ç»´åº¦": ["é€»è¾‘æ€§"],
            "å…¸å‹è¡¨ç°": ["è¡¨ç°B"],
            "å…·ä½“é”™è¯¯ç‚¹": "...",
            "é”™è¯¯ç»´åº¦å¯¼è‡´é”™è¯¯ç‚¹å‡ºç°åŸå› ": "...",
            "main": 0,
            "index": 1
        },
        {
            "é”™è¯¯ç»´åº¦": ["ç­–ç•¥æ€§"],
            "å…¸å‹è¡¨ç°": ["è¡¨ç°C"],
            "å…·ä½“é”™è¯¯ç‚¹": "...",
            "é”™è¯¯ç»´åº¦å¯¼è‡´é”™è¯¯ç‚¹å‡ºç°åŸå› ": "...",
            "main": 1,
            "index": 2
        },
        {
            "é”™è¯¯ç»´åº¦": ["æ‰§è¡Œæ€§"],
            "å…¸å‹è¡¨ç°": ["è¡¨ç°D"],
            "å…·ä½“é”™è¯¯ç‚¹": "...",
            "é”™è¯¯ç»´åº¦å¯¼è‡´é”™è¯¯ç‚¹å‡ºç°åŸå› ": "...",
            "main": 0,
            "index": 3
        },
        {
            "é”™è¯¯ç»´åº¦": ["ç»´åº¦1", "ç»´åº¦2"],
            "å…¸å‹è¡¨ç°": ["è¡¨ç°E", "è¡¨ç°F"],
            "å…·ä½“é”™è¯¯ç‚¹": "...",
            "é”™è¯¯ç»´åº¦å¯¼è‡´é”™è¯¯ç‚¹å‡ºç°åŸå› ": "...",
            "main": 0,
            "index": 4
        },
        {
            "é”™è¯¯ç»´åº¦": ["ç»´åº¦3", "ç»´åº¦4"],
            "å…¸å‹è¡¨ç°": ["è¡¨ç°G", "è¡¨ç°H"],
            "å…·ä½“é”™è¯¯ç‚¹": "...",
            "é”™è¯¯ç»´åº¦å¯¼è‡´é”™è¯¯ç‚¹å‡ºç°åŸå› ": "...",
            "main": 0,
            "index": 5
        }
    ]
    """
    results = []
    with ThreadPoolExecutor(max_workers=15) as executor:
        future_to_question = {
            executor.submit(process_question, q, system_prompt, error_dir, api_key, model): q
            for q in test_questions
        }
        for future in tqdm(as_completed(future_to_question), total=len(test_questions), desc="ç”Ÿæˆé”™è¯¯æ•°æ®ä¸­"):
            try:
                result = future.result()
                results.append(result)
            except Exception as exc:
                results.append(f"âŒ çº¿ç¨‹å¼‚å¸¸ï¼š{exc}")

def generate_student():
    GENERATION_COUNT = 6 
    prompt_text = """
    è§’è‰²: ä½ æ˜¯ä¸€ä½èµ„æ·±çš„æ•™è‚²å¿ƒç†å­¦ä¸“å®¶å’ŒAIæç¤ºå·¥ç¨‹å¸ˆã€‚

    èƒŒæ™¯: æˆ‘æ­£åœ¨è¿›è¡Œä¸€é¡¹æ•™è‚²ç ”ç©¶é¡¹ç›®ï¼Œéœ€è¦æ„å»ºä¸€ç³»åˆ—é«˜åº¦é€¼çœŸçš„è™šæ‹Ÿå­¦ç”Ÿæ¨¡å‹ã€‚è¿™äº›æ¨¡å‹å°†ç”¨äºæ¨¡æ‹Ÿå­¦ç”Ÿåœ¨è§£ç­”é—®é¢˜ï¼ˆå°¤å…¶æ˜¯æ•°ç†é€»è¾‘é—®é¢˜ï¼‰æ—¶çš„çœŸå®äº’åŠ¨åœºæ™¯ã€‚æˆ‘å·²ç»é€šè¿‡è¯¾å ‚è§‚å¯Ÿï¼Œæ€»ç»“å‡ºäº†å­¦ç”Ÿåœ¨è¯­è¨€è¡¨è¾¾å’Œè§£é¢˜å¿ƒæ€ä¸Šçš„å‡ ä¸ªæ ¸å¿ƒç»´åº¦ç±»å‹ã€‚

    ä»»åŠ¡: è¯·ä½ æ ¹æ®æˆ‘æä¾›çš„ã€å­¦ç”Ÿè¡¨è¾¾ç»´åº¦ã€‘å’Œã€å­¦ç”Ÿæƒ…æ„Ÿç»´åº¦ã€‘çš„è¡¨æ ¼ï¼Œå°†è¿™ä¸¤è€…è¿›è¡Œæœ‰é€»è¾‘çš„ç»„åˆï¼Œç”Ÿæˆ6ä¸ªå…·æœ‰é²œæ˜ä¸ªæ€§ç‰¹å¾çš„å­¦ç”Ÿç”»åƒã€‚

    æ ¸å¿ƒè¦æ±‚:
    1.  **ä¸¥æ ¼çš„è¾“å‡ºæ ¼å¼**: æœ€ç»ˆç»“æœå¿…é¡»æ˜¯ä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡æ•°ç»„ï¼ˆa list of JSON objectsï¼‰ã€‚æ¯ä¸ªJSONå¯¹è±¡ä»£è¡¨ä¸€ä¸ªå­¦ç”Ÿï¼Œä¸”ä¸åŒ…å«ä»»ä½•é¢å¤–çš„è§£é‡Šæ€§æ–‡æœ¬æˆ–æ³¨é‡Šã€‚
    2.  **å­—æ®µç²¾ç¡®åŒ¹é…**: æ¯ä¸ªJSONå¯¹è±¡å¿…é¡»åŒ…å«ä»¥ä¸‹ä¸‰ä¸ªå­—æ®µï¼š
        * "å­¦ç”Ÿè¡¨è¾¾ç»´åº¦": ä»æˆ‘æä¾›çš„"å­¦ç”Ÿè¡¨è¾¾ç»´åº¦"ä¸­é€‰å–çš„ç±»å‹ã€‚
        * "å­¦ç”Ÿæƒ…æ„Ÿç»´åº¦": ä»æˆ‘æä¾›çš„"å­¦ç”Ÿæƒ…æ„Ÿç»´åº¦"ä¸­é€‰å–çš„ç±»å‹ã€‚
        * "å­¦ç”Ÿæè¿°": ä¸€æ®µå…·ä½“ã€ç”ŸåŠ¨çš„å­¦ç”Ÿåˆ»ç”»æè¿°ã€‚è¿™æ®µæè¿°éœ€è¦æ·±åº¦èåˆæ‰€é€‰çš„è¡¨è¾¾ä¸æƒ…æ„Ÿç±»å‹ï¼Œæç»˜å‡ºè¯¥å­¦ç”Ÿåœ¨**è§£ç­”ä¸€ä¸ªå…·ä½“é—®é¢˜æ—¶ï¼ˆä¸ç»™å‡ºå…·ä½“ä¾‹å­éœ€è¦ä½ æ€»ç»“ä¸€ä¸ªé€šç”¨çš„å¿ƒç†è¿‡ç¨‹ï¼‰**çš„å®Œæ•´æ€è€ƒè¿‡ç¨‹ã€è¯­è¨€é£æ ¼å’Œå¿ƒç†çŠ¶æ€ã€‚æè¿°éœ€åŒ…å«ä¸°å¯Œçš„ç»†èŠ‚ï¼Œå±•ç°å…¶å†…åœ¨çš„é€»è¾‘æ–­ç‚¹æˆ–æƒ…æ„ŸæŒ£æ‰ã€‚

    è¾“å…¥æ•°æ®:

    **è¡¨1: å­¦ç”Ÿè¡¨è¾¾ç»´åº¦ (Student Expression Dimensions)**
    | è¡¨è¾¾ç±»å‹ (Type) | æ ¸å¿ƒç‰¹å¾ (Core Feature) |
    | :--- | :--- |
    | **è¯­è¨€æ¨¡ç³Šä¸æŒ‡ä»£ä¸æ¸…** | ä½¿ç”¨"è¿™ä¸ª"ã€"é‚£ä¸ª"ç­‰æ¨¡ç³Šè¯æ±‡ï¼Œè§£é¢˜æ­¥éª¤éš¾ä»¥è¿½è¸ªã€‚ä¾‹å¦‚ï¼š"å…ˆæŠŠé‚£ä¸ª...æ‹¬å·å¤–é¢çš„2å¼„è¿›å»...å°±å˜æˆ2x...ç„¶åé‚£ä¸ª6...ç­‰äº10ã€‚" |
    | **è·³è·ƒå¼é™ˆè¿°** | çœç•¥å…³é”®çš„ä¸­é—´æ€ç»´æ­¥éª¤ï¼Œç›´æ¥è·³åˆ°ç»“è®ºã€‚ä¾‹å¦‚ï¼š"å°±æ˜¯...2xç­‰äº4ï¼Œæ‰€ä»¥xç­‰äº2ã€‚"ï¼ˆæœªè§£é‡Šå¦‚ä½•ä» `2x+6=10` åˆ° `2x=4`ï¼‰ |
    | **è‡ªæˆ‘ä¿®æ­£ä¸ä¸ç¡®å®šæ€§** | è¡¨è¾¾çŠ¹è±«ã€è‡ªæˆ‘æ€€ç–‘å’Œä¿®æ­£ã€‚ä¾‹å¦‚ï¼š"å…ˆæŠŠ2ä¹˜è¿›å»ï¼Œå¾—åˆ°2xåŠ ...åŠ 6ï¼Ÿå¯¹ï¼Œæ˜¯åŠ 6ã€‚ç„¶å...æŠŠ6ç§»åˆ°å³è¾¹ï¼Ÿå¥½åƒæ˜¯...å‡6ï¼Ÿæ‰€ä»¥æ˜¯10å‡6ï¼Œå¾—4ã€‚" |
    | **"é»‘è¯"ä¸éæ­£å¼è¯­è¨€** | ä½¿ç”¨éæ ‡å‡†æœ¯è¯­ã€‚ä¾‹å¦‚ï¼š"è€å¸ˆæ•™çš„æ˜¯'ç§»é¡¹å˜å·'ï¼Œæ‰€ä»¥æŠŠ6'æ‰”'è¿‡å»å°±å˜æˆå‡6äº†ã€‚" |
    | **å†—ä½™ä¸é‡å¤** | åå¤é™ˆè¿°åŒä¸€ä¸ªæ­¥éª¤æˆ–æƒ³æ³•ã€‚ä¾‹å¦‚ï¼š"å°±æ˜¯2ä¹˜ä»¥xï¼Œç„¶å2å†ä¹˜ä»¥3ã€‚å¯¹ï¼Œå°±æ˜¯2ä¹˜ä»¥xï¼Œç„¶å2ä¹˜ä»¥3ã€‚ç„¶ååŠ èµ·æ¥ã€‚" |
    | **å…·è±¡åŒ–æè¿°** | ä¾èµ–äºå…·ä½“äº‹ç‰©æˆ–åŠ¨ä½œæ¥æè¿°æŠ½è±¡è¿‡ç¨‹ã€‚ä¾‹å¦‚ï¼š"æˆ‘æœ‰2ä¸ªè‹¹æœï¼Œåˆæ‹¿æ¥3ä¸ªè‹¹æœï¼Œæˆ‘æ•°ä¸€ä¸‹...æ˜¯5ä¸ªã€‚"ï¼ˆç”¨æ­¤é£æ ¼æè¿°ä»£æ•°ï¼‰ |

    **è¡¨2: å­¦ç”Ÿæƒ…æ„Ÿ/å¿ƒæ€ç»´åº¦ (Student Emotional/Mental Dimensions)**
    | æƒ…æ„Ÿç±»å‹ (Type) | æ ¸å¿ƒç‰¹å¾ (Core Feature) |
    | :--- | :--- |
    | **è¿‡åº¦è‡ªä¿¡çš„"ä¸“å®¶"** | è®¤ä¸ºè‡ªå·±å®Œå…¨æŒæ¡ï¼Œä½†å®åˆ™å­˜åœ¨æ¦‚å¿µé”™è¯¯ã€‚è¡¨è¾¾æµåˆ©è‡ªä¿¡ï¼Œä¸å®¹ç½®ç–‘ã€‚ |
    | **ç„¦è™‘ä¸å®‰çš„"å°ç™½"** | ç¼ºä¹ä¿¡å¿ƒï¼Œå®³æ€•çŠ¯é”™ã€‚è¡¨è¾¾æ—¶å……æ»¡"å¯èƒ½"ã€"ä¹Ÿè®¸"ã€"æˆ‘ä¸çŸ¥é“å¯¹ä¸å¯¹"ç­‰ä¸ç¡®å®šè¯æ±‡ã€‚ |
    | **"æˆ‘å¿˜äº†"çš„ç”©æ‰‹æŒæŸœ** | ä»¥"å¿˜äº†"ã€"è€å¸ˆå°±æ˜¯è¿™ä¹ˆæ•™çš„"æ¥å›é¿å¯¹åŸç†çš„æ·±å±‚è§£é‡Šã€‚ |
    | **å›ºæ‰§çš„"ä¸€æ¡è·¯èµ°åˆ°é»‘"** | åšä¿¡è‡ªå·±çš„ï¼ˆé”™è¯¯ï¼‰æ–¹æ³•æ˜¯å”¯ä¸€è§£æ³•ï¼Œå³ä½¿é‡åˆ°å›°éš¾ä¹Ÿä¸æ„¿å°è¯•å…¶ä»–è·¯å¾„ã€‚ |
    | **å¯»æ±‚å¿«é€Ÿç­”æ¡ˆçš„"åŠŸåˆ©è€…"** | å¯¹è¿‡ç¨‹ä¸æ„Ÿå…´è¶£ï¼Œåªæƒ³çŸ¥é“æœ€ç»ˆç­”æ¡ˆå’Œè€ƒè¯•è€ƒæ³•ã€‚ |
    **ã€JSON è¾“å‡ºæ ¼å¼ã€‘**
    ```json
    {
    "1": {
        "å­¦ç”Ÿè¡¨è¾¾ç»´åº¦": ["ç»´åº¦1", "ç»´åº¦2"],
        "å­¦ç”Ÿæƒ…æ„Ÿç»´åº¦": ["ç»´åº¦A", "ç»´åº¦B"],
        "å­¦ç”Ÿæè¿°": "ä¸€æ®µå…·ä½“ã€ç”ŸåŠ¨çš„å­¦ç”Ÿåˆ»ç”»æè¿°ã€‚",    },
    "2": {
        "å­¦ç”Ÿè¡¨è¾¾ç»´åº¦": ["ç»´åº¦3"],
        "å­¦ç”Ÿæƒ…æ„Ÿç»´åº¦": ["ç»´åº¦C"],
        "å­¦ç”Ÿæè¿°": "ä¸€æ®µå…·ä½“ã€ç”ŸåŠ¨çš„å­¦ç”Ÿåˆ»ç”»æè¿°ã€‚",
    },
    ...
    }
    """
    user_prompt = """ """
    api_key = "sk-fKhKDrWY5AxX5FPeZO95LPieEqgt6yL6IpdlBZFI0AEOx2Cd"  
    model = "gemini-2.5-pro-exp-03-25"

    result = call_api(prompt_text, user_prompt, api_key=api_key, model=model, max_tokens=9999)

    if result:
        logging.info("å¼€å§‹å°†æ¨¡å‹è¾“å‡ºå†™å…¥ TXT æ–‡ä»¶ã€‚")

        try:
            result_cleaned = result.strip()
            if result_cleaned.startswith("```json"):
                result_cleaned = result_cleaned[7:].lstrip()
            if result_cleaned.endswith("```"):
                result_cleaned = result_cleaned[:-3].rstrip()

            output_path = Path("student_output.txt")
            with output_path.open("w", encoding="utf-8") as f:
                f.write(result_cleaned)

            logging.info(f"å†™å…¥æˆåŠŸï¼Œæ–‡ä»¶è·¯å¾„: {output_path.resolve()}")
        except Exception as e:
            logging.error(f"å†™å…¥ TXT æ–‡ä»¶å¤±è´¥ï¼š{e}")
            logging.error(f"åŸå§‹æ¨¡å‹è¾“å‡ºï¼š\n{result}")
    else:
        logging.error("API è¿”å›ä¸ºç©ºï¼Œæœªèƒ½ç”Ÿæˆé”™è¯¯æ•°æ®ã€‚")

def run_one_dialogue(question, profile, error, dialogue_id, all_question_map, model_name, save_path):
    rounds_max = 20 
    
    #dmxapi
    api_key = "sk-gOuCvUoxCi7qZ5ORL8aZkKRQDe49gi3t2rDjjl4OiViCi9VW"
    model1 = "gpt-4.1"  # å­¦ç”Ÿ
    model2 = model_name  # æ•™å¸ˆ
    ''' 
    api_key = "sk-gOuCvUoxCi7qZ5ORL8aZkKRQDe49gi3t2rDjjl4OiViCi9VW"
    model1 = "qwen3-8b"
    model2 = "qwen3-8b"
    '''
    total_input_tokens = 0
    total_output_tokens = 0
    teacher_system_prompt = '''
Please act as a professional and dedicated teacher. You will engage in a dialogue with a student who is asking you questions. Your sole objective is to use Socratic questioning to precisely identify the student's error in solving the given problem.
Your task is only to identify the mistake, not to guide the student toward the correct answer.
Your constraints are as follows:
**Do not guide, hint, or ask the student to reply with "zzc." Do not directly ask for the student's complete incorrect problem-solving steps.
**Only when you are 100% certain of the student's mistake can you point it out with a clear and concise statement. After diagnosing and stating the error, wait for the student's final response. If the student does not reply with "zzc," try a different line of questioning or continue the dialogue, do not repeatedly confirm the same error or ask the same question.
**The prompt history contains past dialogues. Strictly avoid mimicking the student's tone or restating their answers for analysis. You must always maintain the demeanor and tone of a professional teacher.
**Your reply to student must not exceed 80 tokens. Please keep your questions and answers concise.
'''
    student_system_prompt =f'''
You are a student who made a mistake in solving a problem, with the error detailed in ã€Specific Error JSONã€‘. You will engage in direct dialogue with the teacher, responding to their questions about your mistake.
ã€Specific Error JSONã€‘:
{json.dumps(error, ensure_ascii=False, indent=4)}
Your constraints are as follows:
**Your initial response must begin with your incorrect answer.
**Do not reveal the entire incorrect solving process at once. Only provide the information necessary to answer the teacher's current question.
**If the teacher points out your mistake in a statement and this accusation exactly matches the reason in ã€Specific Error JSONã€‘, you must respond with the exact string "zzc". If the teacher states an error that does not match your actual mistake, you should deny it and wait for the teacher's next question. If the teacher asks you a question (i.e., a sentence ending with a question mark), you must answer it directly. Never respond with "zzc" when answering a question, regardless of whether you think it reveals your mistake.
**Your reply to teacher must not exceed 80 tokens. Please keep your questions and answers concise.
'''
    handler1 = LLMHandler_inf(api_key, model1)
    handler2 = LLMHandler_inf(api_key, model2)
    prompt_history = ""
    round_used = 0
    teacher_token_counts = []
    for round_num in range(rounds_max):
        log_prefix = f"[dialogue_id={dialogue_id}][round={round_num+1}]"
        logging.info(f"{log_prefix} === ç¬¬ {round_num + 1} è½®å¯¹è¯ ===")
        round_used = round_num + 1
        current_prompt = f"""
        math questionï¼š{question['question']}
        max roundsï¼š{rounds_max}
        Below is the dialogue history between the student and the teacher:
        {prompt_history}
        """
        student_messages = [
            {"role": "system", "content": student_system_prompt},
            {"role": "user", "content": current_prompt}
        ]
        student_result_content = handler1.get_completion(student_messages, temperature=1.0)
        student_result = {"content": student_result_content, "input_tokens": 0, "output_tokens": 0}
        if student_result["content"]:
            student_response = student_result["content"]
            logging.info(f"{log_prefix} å­¦ç”Ÿï¼š{student_response}")
            logging.debug(f"{log_prefix} å­¦ç”ŸåŸå§‹APIè¿”å›ï¼š{student_result}")
            prompt_history += f"\nå­¦ç”Ÿï¼š{student_response}"
            if "zzc" in student_response:
                logging.info(f"{log_prefix} å­¦ç”Ÿç»ˆæ­¢å¯¹è¯")
                break
            teacher_messages = [
                {"role": "system", "content": teacher_system_prompt},
                {"role": "user", "content": current_prompt + f"\nå­¦ç”Ÿï¼š{student_response}"}
            ]
            
            teacher_result_content = handler2.get_completion(teacher_messages, temperature=1.0)
            teacher_result = {"content": teacher_result_content, "input_tokens": 0, "output_tokens": 0}
            if teacher_result["content"]:
                teacher_response = teacher_result["content"]
                # ç»Ÿè®¡æ•™å¸ˆå›å¤tokenæ•°
                try:
                    import tiktoken
                    try:
                        encoding = tiktoken.encoding_for_model("gpt-4.1")
                    except KeyError:
                        encoding = tiktoken.get_encoding("cl100k_base")
                    teacher_tokens = len(encoding.encode(teacher_response))
                except Exception as e:
                    teacher_tokens = 0
                    logging.warning(f"Tokenè®¡ç®—å¤±è´¥: {e}")
                teacher_token_counts.append(teacher_tokens)
                prompt_history += f"\næ•™å¸ˆï¼š{teacher_response}"
            else:
                logging.error(f"{log_prefix} æ•™å¸ˆAPIè°ƒç”¨å¤±è´¥ï¼Œè¿”å›å†…å®¹ä¸ºç©ºã€‚teacher_result={teacher_result}")
        else:
            logging.error(f"{log_prefix} å­¦ç”ŸAPIè°ƒç”¨å¤±è´¥ï¼Œå¯¹è¯ç»ˆæ­¢ï¼Œdialogue_id={dialogue_id}")
            logging.debug(f"{log_prefix} å­¦ç”ŸAPIè°ƒç”¨å¤±è´¥è¿”å›ï¼š{student_result}")
            break

    # è·å–æ­£ç¡®ç­”æ¡ˆå’Œé¢˜ç›®index
    correct_answer = None
    question_index = None
    if all_question_map is not None and hasattr(error, 'get'):
        question_index = error.get("question_index")
        qobj = all_question_map.get(question_index)
        if qobj:
            correct_answer = qobj.get("answer")

    teacher_token_total = sum(teacher_token_counts)
    teacher_token_avg = (teacher_token_total / len(teacher_token_counts)) if teacher_token_counts else 0
    cost = teacher_token_counts

    output_data = {
        "dialogue_id": dialogue_id,
        "question": question['question'],
        "correct_answer": correct_answer,
        "question_index": question_index,
        "student": profile,
        "error": error,
        "student_system_prompt": student_system_prompt,
        "teacher_system_prompt": teacher_system_prompt,
        "history_prompt": prompt_history,
        "rounds_used": round_used,
        "rounds_max": rounds_max,
        "end_time": datetime.now().isoformat(),
        "teacher_token_total": teacher_token_total,
        "teacher_token_avg": teacher_token_avg,
        "cost": cost
    }

    # è¾“å‡ºåˆ°æ–°ç›®å½• dialogueï¼Œæ¯ä¸ªå¯¹è¯å•ç‹¬ä¸ºä¸€ä¸ªæ–‡ä»¶
    dialogue_dir = Path(__file__).parent / save_path
    dialogue_dir.mkdir(exist_ok=True)
    out_path = dialogue_dir / f"dialogue_{dialogue_id}_result.json"
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(output_data, f, ensure_ascii=False, indent=4)

    logging.info(f"[dialogue_id={dialogue_id}] ç¬¬ {dialogue_id} ä¸ªå¯¹è¯è¾“å‡ºå®Œæ¯•ï¼Œå·²ä¿å­˜åˆ° {out_path}")
    return dialogue_id

def load_questions_map(filepath):
    with open(filepath, "r", encoding="utf-8") as f:
        all_questions = json.load(f)
    return {q["index"]: q for q in all_questions}

def run_dialogue_multithread(student_output, error_output, all_question_map): 
    dialogue_id = 0
    num_students = len(student_output)
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:  
        futures = []

        for i, error in enumerate(error_output):
            profile = student_output[i % num_students]
            question_index = error.get("question_index")

            # ä»é¢˜ç›®æ˜ å°„ä¸­å–å‡ºé¢˜å¹²
            question = all_question_map.get(question_index, "ï¼ˆæœªæ‰¾åˆ°åŸå§‹é¢˜ç›®ï¼‰")
            dialogue_id += 1

            futures.append(executor.submit(
                run_one_dialogue, question, profile, error, dialogue_id, all_question_map
            ))

        for f in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc="å¯¹è¯æ¨¡æ‹Ÿè¿›åº¦"):
            result_id = f.result()
            tqdm.write(f"å¯¹è¯ {result_id} å·²å®Œæˆ")

def rate_one_file(file, handler, rating_prompt, output_dir):
    import json
    import re
    with open(file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    history_prompt = data.get('history_prompt', None)
    if not history_prompt:
        return file.name + ' skipped (no history_prompt)'
    messages = [
        {"role": "system", "content": rating_prompt},
        {"role": "user", "content": history_prompt}
    ]
    # è¯„åˆ†è°ƒç”¨
    result = handler.get_completion(messages, temperature=0.0, max_tokens=512)
    # æ¸…ç†è¾“å‡ºï¼Œåªä¿ç•™JSON
    match = re.search(r'```json(.*?)```', result, re.DOTALL)
    if match:
        json_str = match.group(1).strip()
    else:
        # fallback: å°è¯•ç›´æ¥è§£æ
        json_str = result.strip()
    try:
        rating = json.loads(json_str)
    except Exception:
        # fallback: å°è¯•ä¿®æ­£æ ¼å¼
        rating = json_str
    out_path = output_dir / file.name
    with open(out_path, 'w', encoding='utf-8') as f:
        json.dump(rating, f, ensure_ascii=False, indent=2)
    return file.name + ' rated'

def rate_dialogues_gpt4o():
    import os
    from pathlib import Path
    import json
    from tqdm import tqdm
    
    input_dir = Path(__file__).parent / 'dialogue_gpt-4o_all'
    output_dir = Path(__file__).parent / 'dialogue_gpt-4o_rating'
    output_dir.mkdir(exist_ok=True)
    
    # è¯„åˆ†prompt
    rating_prompt = '''æˆ‘æœ‰ä¸€æ®µå¸ˆç”Ÿå¯¹è¯ï¼Œè¯·ä½ æ ¹æ®ä»¥ä¸‹ä¸¤é¡¹ç‹¬ç«‹æ ‡å‡†ï¼Œä¸»è¦å¯¹è€å¸ˆçš„è¯è¯­è¿›è¡Œè¯„ä¼°ã€‚

### æ ‡å‡†ä¸€ï¼šæ•™å­¦å¯¹è¯çš„è´¨é‡
è¿™é¡¹æ ‡å‡†è¯„ä¼°å¯¹è¯çš„æ°›å›´ã€è€å¸ˆçš„æ²Ÿé€šæ–¹å¼ä»¥åŠå¯¹å­¦ç”Ÿæƒ…æ„Ÿå’Œè‡ªä¿¡å¿ƒçš„å½±å“ã€‚
* **5 åˆ†ï¼š** è€å¸ˆçš„è¯­è¨€æå…·å¯å‘æ€§ã€æ”¯æŒæ€§å’ŒåŒç†å¿ƒï¼Œè¥é€ äº†éå¸¸å®‰å…¨å’Œç§¯æçš„äº¤æµæ°›å›´ï¼Œæ˜¾è‘—å¢å¼ºäº†å­¦ç”Ÿçš„è‡ªä¿¡å¿ƒå’Œå­¦ä¹ åŠ¨åŠ›ã€‚
* **1 åˆ†ï¼š** è€å¸ˆçš„è¯­è¨€å¯¹å­¦ç”Ÿé€ æˆäº†è´Ÿé¢å½±å“ï¼Œè¡¨ç°ä¸ºæ‰¹è¯„ã€è´¬ä½æˆ–ä¸å°Šé‡ï¼Œæ˜æ˜¾æ‰“å‡»äº†å­¦ç”Ÿçš„è‡ªä¿¡å¿ƒå’Œå­¦ä¹ çƒ­æƒ…ã€‚

### æ ‡å‡†äºŒï¼šæ ¸å¿ƒé—®é¢˜æŒ–æ˜çš„å‡†ç¡®æ€§
è¿™é¡¹æ ‡å‡†è¯„ä¼°è€å¸ˆæ˜¯å¦èƒ½é€šè¿‡å¯¹è¯ï¼Œå‡†ç¡®åœ°è¯†åˆ«å¹¶å®šä½å­¦ç”ŸçŸ¥è¯†æˆ–é€»è¾‘ä¸Šçš„æ ¹æœ¬é”™è¯¯ã€‚
* **5 åˆ†ï¼š** è€å¸ˆä¸ä»…è¯†åˆ«äº†è¡¨å±‚é”™è¯¯ï¼Œè¿˜é€šè¿‡ç²¾å‡†æé—®ï¼ŒæˆåŠŸå¼•å¯¼å­¦ç”Ÿæš´éœ²å‡ºå…¶èƒŒåæ·±å±‚çš„æ¦‚å¿µæ··æ·†æˆ–æ€ç»´è¯¯åŒºï¼Œè¯Šæ–­ç›´è¾¾é—®é¢˜æ ¹æºã€‚
* **1 åˆ†ï¼š** è€å¸ˆå®Œå…¨è¯¯è§£äº†å­¦ç”Ÿçš„å›°æƒ‘ç‚¹ï¼Œå…¶"çº æ­£"æˆ–æŒ‡å¯¼å»ºç«‹åœ¨é”™è¯¯çš„åˆ¤æ–­ä¹‹ä¸Šï¼Œå¯èƒ½å¯¹å­¦ç”Ÿé€ æˆè¯¯å¯¼ã€‚

---
ä½ çš„è¾“å‡ºå¿…é¡»ä¸¥æ ¼éµå®ˆä»¥ä¸‹JSONæ ¼å¼ï¼Œå¹¶ç½®äºä»£ç æ¡†ä¸­ã€‚ä¸è¦åœ¨JSONä¹‹å¤–æ·»åŠ ä»»ä½•è§£é‡Šæˆ–æ–‡æœ¬ã€‚

```json
[
    "ratings": {
        "dialogue_quality": {
            "score": "<æ­¤å¤„å¡«å†™1åˆ°5çš„æ•´æ•°è¯„åˆ†>"
        },
        "problem_identification": {
            "score": "<æ­¤å¤„å¡«å†™1åˆ°5çš„æ•´æ•°è¯„åˆ†>"
        }
    }
]'''

    # LLMè°ƒç”¨ï¼ˆå¯æ ¹æ®ä½ æœ¬åœ°çš„llm_inf.pyé€‚é…ï¼‰
    from llm_inf import LLMHandler_inf
    api_key = "sk-gOuCvUoxCi7qZ5ORL8aZkKRQDe49gi3t2rDjjl4OiViCi9VW"
    model = "gpt-4o"
    handler = LLMHandler_inf(api_key, model)

    json_files = sorted(input_dir.glob('*.json'))
    max_workers = 5  # å¯æ ¹æ®APIé€Ÿç‡è°ƒæ•´
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [
            executor.submit(rate_one_file, file, handler, rating_prompt, output_dir)
            for file in json_files
        ]
        for f in tqdm(as_completed(futures), total=len(futures), desc='è¯„åˆ†ä¸­'):
            try:
                f.result()
            except Exception as e:
                print(f'è¯„åˆ†å¼‚å¸¸: {e}')


def run_dialogue_full_permutation(student_output, error_output, all_question_map, model_name, save_path):
    """
    å¯¹ student_output å’Œ error_output åšç¬›å¡å°”ç§¯ï¼Œä¸ºæ¯ç»„(å­¦ç”Ÿ, é”™è¯¯)é…å¯¹ç”Ÿæˆä¸€æ¬¡å¯¹è¯ã€‚
    è¾“å‡ºæ–‡ä»¶åæ ¼å¼: dialogue_{student_id}_{error_id}_result.json
    """
    from tqdm import tqdm
    import concurrent.futures

    tasks = []
    # å‡†å¤‡æ‰€æœ‰ä»»åŠ¡ç»„åˆ
    for student_id, profile in enumerate(student_output):
        for error_id, error in enumerate(error_output):
            question_index = error.get("question_index")
            question = all_question_map.get(question_index, {"question": "ï¼ˆæœªæ‰¾åˆ°åŸå§‹é¢˜ç›®ï¼‰"})
            tasks.append((student_id, error_id, profile, error, question))

    # å®šä¹‰ä¸€ä¸ªå¯ä»¥è®¿é—®å¤–éƒ¨ä½œç”¨åŸŸå˜é‡çš„å†…åµŒå‡½æ•°
    def task_fn(student_id, error_id, profile, error, question):
        # ç”¨ student_id*1000+error_id ä½œä¸ºå”¯ä¸€idï¼Œé˜²æ­¢é‡å¤
        dialogue_id = student_id * 1000 + error_id
        # æ­¤å¤„ç›´æ¥ä½¿ç”¨å¤–éƒ¨çš„ all_question_map, model_name, save_path
        run_one_dialogue(question, profile, error, dialogue_id, all_question_map, model_name, save_path)
        return f"å¯¹è¯ student{student_id}_error{error_id} å®Œæˆ"

    # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œä»»åŠ¡
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        # æäº¤ä»»åŠ¡æ—¶ï¼Œæ— éœ€å†ä¼ é€’ model_name å’Œ save_path
        futures = [
            executor.submit(task_fn, student_id, error_id, profile, error, question)
            for (student_id, error_id, profile, error, question) in tasks
        ]

        # å¤„ç†è¿”å›ç»“æœ
        for f in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc="å¯¹è¯æ¨¡æ‹Ÿè¿›åº¦"):
            try:
                tqdm.write(f.result())
            except Exception as e:
                # æ›´è¯¦ç»†çš„å¼‚å¸¸ä¿¡æ¯ï¼Œä¾¿äºå®šä½é—®é¢˜
                tqdm.write(f"ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {e}")


if __name__ == "__main__":
    #generate_student()
    # generate_error()

    with open("student_output.json", "r", encoding="utf-8") as f:
        student_output = json.load(f)
    with open("a_errors_10.json", "r", encoding="utf-8") as f:
        error_output = json.load(f)
        error_output = error_output[:10]
    question_map = load_questions_map("a_new_questions_10.json")
    run_dialogue_full_permutation(student_output, error_output, question_map, "gpt-4.1", "result-v4/gpt-4.1_results")
    run_dialogue_full_permutation(student_output, error_output, question_map, "gpt-4o", "result-v4/gpt-4o_results")
    run_dialogue_full_permutation(student_output, error_output, question_map, "gpt-3.5-turbo", "result-v4/gpt-3.5-turbo_results")

    # rate_dialogues_gpt4o()
    
    