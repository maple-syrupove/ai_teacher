import requests
import json
import logging
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from datetime import datetime
from tqdm import tqdm
import tiktoken
import openai
import tiktoken
import time
import os
from llm_inf import LLMHandler_inf
import re
# ç§»é™¤Qwenåˆ†è¯å™¨ç›¸å…³å†…å®¹ï¼Œæ— éœ€from transformers import AutoTokenizer

logging.basicConfig(filename='main-gemini.log', level=logging.DEBUG, encoding='utf-8', format='%(asctime)s - %(levelname)s - %(message)s')


def num_tokens_from_messages(messages, model="gpt-4o"):
    """è®¡ç®—æ¶ˆæ¯æ€» token æ•°ï¼ˆé€‚é… OpenAI chat æ ¼å¼ï¼‰"""
    try:
        encoding = tiktoken.encoding_for_model(model)
    except KeyError:
        encoding = tiktoken.get_encoding("cl100k_base")

    # ä»¥ä¸‹æ˜¯ä¼°ç®—è§„åˆ™ï¼ˆåŸºäº OpenAI å®˜æ–¹æè¿°ï¼‰
    num_tokens = 0
    for message in messages:
        num_tokens += 4  # æ¯æ¡ message åŒ…æ‹¬ role å’Œ metadata
        for key, value in message.items():
            num_tokens += len(encoding.encode(value))
    num_tokens += 2  # åŒ…æ‹¬ assistant å›å¤å¼€å¤´
    return num_tokens

def call_api(system_prompt, prompt, api_key, model, max_tokens=2048, max_retries=10):
    url = "https://www.dmxapi.com/v1/chat/completions"
    headers = {
        'Accept': 'application/json',
        'Authorization': f'Bearer {api_key}',
        'User-Agent': 'DMXAPI/1.0.0 (https://www.dmxapi.com)',
        'Content-Type': 'application/json'
    }

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": prompt}
    ]

    is_qwen = "qwen" in model.lower()

    input_tokens = 0
    if not is_qwen:
        try:
            input_tokens = num_tokens_from_messages(messages, model)
            logging.info(f"ğŸ”¢ è¾“å…¥ Token æ•°: {input_tokens}")
        except Exception as e:
            logging.warning(f"è¾“å…¥ Token è®¡ç®—å¤±è´¥: {e}")

    payload = json.dumps({
        "model": model,
        "messages": messages,
        "temperature": 1.0
    })

    for attempt in range(max_retries):
        try:
            response = requests.post(url, headers=headers, data=payload, timeout=240)
            response.raise_for_status()
            response_json = response.json()

            if 'choices' in response_json and response_json['choices']:
                content = response_json['choices'][0]['message']['content']
                logging.info("æ¨¡å‹è¿”å›ç»“æœï¼š")
                logging.info(content)

                output_tokens = 0
                if not is_qwen:
                    try:
                        output_tokens = len(tiktoken.encoding_for_model(model).encode(content))
                        logging.info(f"ğŸ”¢ è¾“å‡º Token æ•°: {output_tokens}")
                        logging.info(f"ğŸ§® æ€» Token æ•°: {input_tokens + output_tokens}")
                    except Exception as e:
                        logging.warning(f"è¾“å‡º Token è®¡ç®—å¤±è´¥: {e}")

                return {
                    "content": content,
                    "input_tokens": input_tokens if not is_qwen else 0,
                    "output_tokens": output_tokens if not is_qwen else 0
                }
            else:
                logging.warning("API å“åº”ä¸­æœªåŒ…å« 'choices'")

        except requests.exceptions.RequestException as e:
            logging.error(f"APIè¯·æ±‚å¼‚å¸¸ (å°è¯• {attempt + 1}/{max_retries}): {e}\nè¯·æ±‚payload: {payload}")
        except (json.JSONDecodeError, KeyError) as e:
            logging.error(f"å“åº”è§£æå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
        if attempt < max_retries - 1:
            time.sleep(2)

    return {
        "content": None,
        "input_tokens": 0 if is_qwen else input_tokens,
        "output_tokens": 0
    }

def process_question(q, system_prompt, error_dir, api_key, model):
    index = q.get('index', 0)
    user_prompt = f"""
    ä¸‹é¢çš„ã€æ•°å­¦é—®é¢˜ã€‘ç”Ÿæˆç›¸åº”çš„å­¦ç”Ÿé”™è¯¯JSONæ•°æ®:
    {json.dumps(q, ensure_ascii=False, indent=2)}
    """
    handler = LLMHandler_inf(api_key, model)
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    result_content = handler.get_completion(messages, temperature=1.0, max_tokens=9999)
    result = {"content": result_content, "input_tokens": 0, "output_tokens": 0}  # tokenç»Ÿè®¡å¯é€‰
    if result["content"]:
        try:
            result_cleaned = result["content"].strip()
            if result_cleaned.startswith("```json"):
                result_cleaned = result_cleaned[7:].lstrip()
            if result_cleaned.endswith("```"):
                result_cleaned = result_cleaned[:-3].rstrip()
            output_path = error_dir / f"question_{index}_error.json"
            with open(output_path, "w", encoding="utf-8") as f:
                f.write(result_cleaned)
            return f"âœ… å†™å…¥æˆåŠŸï¼šquestion_{index}_error.json"
        except Exception as e:
            return f"âŒ å†™å…¥å¤±è´¥ï¼šquestion_{index}_error.json\né”™è¯¯ï¼š{e}\nè¾“å‡ºï¼š\n{result}"
    else:
        return f"âŒ APIè¿”å›ä¸ºç©ºï¼šindex={index}"

def generate_error():
    test_path = Path(__file__).parent / 'new_questions_100.json'
    with open(test_path, 'r', encoding='utf-8') as f:
        test_questions = json.load(f)

    error_dir = Path(__file__).parent / 'error_math_100'
    error_dir.mkdir(exist_ok=True)


    api_key = "sk-gOuCvUoxCi7qZ5ORL8aZkKRQDe49gi3t2rDjjl4OiViCi9VW"  
    model = "gpt-4.1"
    system_prompt = """
    **è§’è‰²:** ä½ æ˜¯ä¸€ä½ç²¾é€šæ•°å­¦æ•™è‚²å’Œå­¦ç”Ÿå­¦ä¹ å¿ƒç†å­¦çš„ä¸“å®¶ã€‚ä½ æ·±åˆ»ç†è§£å­¦ç”Ÿåœ¨è§£å†³æ•°å­¦é—®é¢˜æ—¶å¯èƒ½çŠ¯çš„å„ç§é”™è¯¯ï¼Œå¹¶èƒ½å‡†ç¡®åœ°å°†è¿™äº›é”™è¯¯ä¸ç‰¹å®šçš„è®¤çŸ¥ç»´åº¦ï¼ˆå¦‚çŸ¥è¯†æ€§ã€é€»è¾‘æ€§ã€ç­–ç•¥æ€§ã€æ‰§è¡Œæ€§ï¼‰è”ç³»èµ·æ¥ã€‚

**ä»»åŠ¡:** æˆ‘å°†ä¸ºä½ æä¾›ä¸€ä¸ªã€é”™è¯¯ç»´åº¦è¡¨æ ¼ã€‘å’Œä¸€ä¸ªå…·ä½“çš„ã€æ•°å­¦é—®é¢˜ã€‘ã€‚è¯·ä½ ä¸¥æ ¼éµå¾ªä»¥ä¸‹è¦æ±‚ï¼Œä¸ºè¯¥é—®é¢˜**å›ºå®šç”Ÿæˆå…­ç§ä¸åŒç±»å‹çš„å­¦ç”Ÿé”™è¯¯**JSONæ•°æ®ã€‚è¿™å…­ç§é”™è¯¯å¿…é¡»ä¸¥æ ¼æŒ‰ç…§**å››ç§å•ç»´åº¦é”™è¯¯**å’Œ**ä¸¤ç§å¤šç»´åº¦å¤åˆé”™è¯¯**çš„ç»“æ„è¿›è¡Œç»„ç»‡ã€‚

**æ ¸å¿ƒè¦æ±‚:**

1.  **å›ºå®šçš„é”™è¯¯ç»“æ„:** ä½ å¿…é¡»ä¸å¤šä¸å°‘ï¼Œæ­£å¥½ç”Ÿæˆå…­ä¸ªé”™è¯¯JSONå¯¹è±¡ã€‚å…¶ç»“æ„å¿…é¡»å¦‚ä¸‹ï¼š
    * å‰å››ä¸ªé”™è¯¯å¯¹è±¡å¿…é¡»æ˜¯**å•ç»´åº¦é”™è¯¯**ï¼Œä¾æ¬¡åˆ†åˆ«å¯¹åº”ã€çŸ¥è¯†æ€§ã€‘ã€ã€é€»è¾‘æ€§ã€‘ã€ã€ç­–ç•¥æ€§ã€‘å’Œã€æ‰§è¡Œæ€§ã€‘è¿™å››ä¸ªç»´åº¦ã€‚æ¯ä¸ªé”™è¯¯éƒ½åº”æ˜¯è¯¥ç»´åº¦çš„å…¸å‹ã€çº¯ç²¹çš„ä½“ç°ã€‚
    * åä¸¤ä¸ªé”™è¯¯å¯¹è±¡å¿…é¡»æ˜¯**å¤šç»´åº¦ï¼ˆæ·±åº¦å¤æ‚ï¼‰é”™è¯¯**ï¼Œæ¯ä¸ªé”™è¯¯å¿…é¡»ç”±è‡³å°‘ä¸¤ä¸ªç»´åº¦å…±åŒå¯¼è‡´ï¼Œç”¨ä»¥æ¨¡æ‹ŸçœŸå®ä¸–ç•Œä¸­å­¦ç”Ÿé”™è¯¯æˆå› çš„å¤æ‚æ€§ã€‚

2.  **æ¨¡æ‹ŸçœŸå®æ€§:** ç”Ÿæˆçš„æ¯ä¸ªé”™è¯¯éƒ½å¿…é¡»é«˜åº¦æ¨¡æ‹ŸçœŸå®å­¦ç”Ÿå¯èƒ½å‡ºç°çš„æ€ç»´å’Œæ“ä½œè¿‡ç¨‹ã€‚åŸå› è§£é‡Šè¦åˆç†ã€æœ‰è¯´æœåŠ›ï¼Œä¸èƒ½ç”Ÿæ¬ç¡¬å¥—è¡¨æ ¼ä¸­çš„å®šä¹‰ã€‚

3.  **å¤šç»´åº¦å½’å› :** å¯¹äºé‚£ä¸¤ä¸ª**å¤šç»´åº¦å¤åˆé”™è¯¯**ï¼Œä½ çš„åˆ†æå¿…é¡»æ¸…æ™°åœ°ä½“ç°å‡ºå¤šä¸ªâ€œé”™è¯¯ç»´åº¦â€æ˜¯å¦‚ä½•å…±åŒä½œç”¨ï¼Œå¯¼è‡´æœ€ç»ˆçš„é”™è¯¯è¡¨ç°çš„ã€‚

4.  **ç²¾ç¡®åˆ°ç‚¹:** "å…·ä½“é”™è¯¯ç‚¹"å¿…é¡»æ˜ç¡®æŒ‡å‡ºåœ¨è§£å†³è¿™ä¸ªç‰¹å®šã€æ•°å­¦é—®é¢˜ã€‘æ—¶ï¼Œå­¦ç”Ÿåœ¨å“ªä¸€æ­¥ã€å“ªä¸ªå…¬å¼ã€å“ªä¸ªæ•°å­—ä¸Šå‡ºé”™äº†ã€‚

5.  **æ·±åº¦åˆ†æ:** "é”™è¯¯ç»´åº¦å¯¼è‡´é”™è¯¯ç‚¹å‡ºç°åŸå› "æ˜¯æ ¸å¿ƒï¼Œéœ€è¦è¯¦ç»†ã€æ¸…æ™°åœ°è§£é‡Šä¸ºä»€ä¹ˆå­¦ç”Ÿä¼šå› ä¸ºè¿™ä¸ª/è¿™äº›ç»´åº¦ä¸Šçš„ç¼ºé™·ï¼Œæœ€ç»ˆå¯¼è‡´äº†é‚£ä¸ªå…·ä½“çš„é”™è¯¯ã€‚

6.  **é‡ç‚¹æ ‡æ³¨:** åœ¨ç”Ÿæˆçš„**å…­ä¸ª**é”™è¯¯ä¸­ï¼Œè¯·æ ¹æ®ä½ çš„ä¸“ä¸šåˆ¤æ–­ï¼Œé€‰æ‹©å‡º**ä¸€ä¸ª**çœŸå®å­¦ç”Ÿæœ€å®¹æ˜“çŠ¯çš„é”™è¯¯ï¼Œå¹¶å°†å…¶`main`å­—æ®µè®¾ç½®ä¸º1ã€‚å…¶ä½™äº”ä¸ªçš„`main`å­—æ®µå¿…é¡»ä¸º0ã€‚

7.  **ä¸¥æ ¼çš„JSONæ ¼å¼:** è¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªåŒ…å«**å…­ä¸ª**å¯¹è±¡çš„ã€æ ‡å‡†çš„ã€æ— æ³¨é‡Šçš„JSONæ•°ç»„ã€‚ä¸è¦æ·»åŠ æ–°çš„keyã€‚"index"ä¸ºä»0å¼€å§‹çš„æ•°å€¼ç¼–å·ã€‚

8.  **ä¿è¯é”™è¯¯:** "å…·ä½“é”™è¯¯ç‚¹"ä¸­å¿…é¡»æ˜ç¡®æŒ‡å‡ºå­¦ç”Ÿå¾—åˆ°çš„é”™è¯¯ç­”æ¡ˆæˆ–è€…æ˜¯æ— æ³•ç»™å‡ºå…·ä½“ç­”æ¡ˆï¼Œé”™è¯¯ç­”æ¡ˆå¿…é¡»ä¸æ­£ç¡®ç­”æ¡ˆä¸åŒã€‚

---

**ã€é”™è¯¯ç»´åº¦è¡¨æ ¼ã€‘**

| é”™è¯¯ç»´åº¦ (Error Dimension) | æ ¸å¿ƒå®šä¹‰ (Core Definition)                                 | å…¸å‹è¡¨ç° (Typical Manifestations)                        |
| -------------------------- | ---------------------------------------------------------- | -------------------------------------------------------- |
| **çŸ¥è¯†æ€§é”™è¯¯ (Knowledge-Based)** | å› æ•°å­¦çŸ¥è¯†æœ¬èº«çš„ç¼ºé™·ã€é—å¿˜æˆ–ç†è§£ä¸æ·±åˆ»å¯¼è‡´çš„é”™è¯¯ã€‚         | æ¦‚å¿µæ··æ·†ã€æ€§è´¨è¯¯è®°ã€å…¬å¼é”™ç”¨ã€å‰ææ¡ä»¶å¿½è§†ã€‚             |
| **é€»è¾‘æ€§é”™è¯¯ (Logical)** | å› è¿èƒŒæ•°å­¦çš„ä¸¥è°¨æ€§ã€æ¨ç†è§„åˆ™å’Œé€»è¾‘å½¢å¼è€Œäº§ç”Ÿçš„é”™è¯¯ã€‚         | å¾ªç¯è®ºè¯ã€åˆ†ç±»è®¨è®ºä¸å®Œå¤‡ã€å·æ¢æ¦‚å¿µã€ä¸ç­‰ä»·å˜æ¢ã€‚         |
| **ç­–ç•¥æ€§é”™è¯¯ (Strategic)** | å› æœªèƒ½é€‰æ‹©æˆ–æ‰§è¡Œæœ‰æ•ˆçš„è§£é¢˜è·¯å¾„å’Œæ€æƒ³æ–¹æ³•è€Œå¯¼è‡´çš„é”™è¯¯ã€‚     | æ¨¡å¼è¯†åˆ«å¤±è´¥ã€ç¼ºä¹æ•´ä½“è§‚ã€æ€ç»´åƒµåŒ–ã€æ— æ³•è½¬åŒ–é—®é¢˜ã€‚       |
| **æ‰§è¡Œæ€§é”™è¯¯ (Execution)** | åœ¨è§£é¢˜çš„å…·ä½“æ“ä½œç¯èŠ‚å‡ºç°çš„å¤±è¯¯ï¼Œå¸¸è¢«ç¬¼ç»Ÿåœ°ç§°ä¸º"ç²—å¿ƒ"ã€‚     | å®¡é¢˜ä¸æ¸…ã€æŠ„å†™é”™è¯¯ã€è®¡ç®—å¤±è¯¯ã€ä¹¦å†™ä¸è§„èŒƒã€‚               |

---

**ã€JSON è¾“å‡ºæ ¼å¼ã€‘**

```json
[
    {
        "é”™è¯¯ç»´åº¦": ["çŸ¥è¯†æ€§"],
        "å…¸å‹è¡¨ç°": ["è¡¨ç°A"],
        "å…·ä½“é”™è¯¯ç‚¹": "...",
        "é”™è¯¯ç»´åº¦å¯¼è‡´é”™è¯¯ç‚¹å‡ºç°åŸå› ": "...",
        "main": 0,
        "index": 0
    },
    {
        "é”™è¯¯ç»´åº¦": ["é€»è¾‘æ€§"],
        "å…¸å‹è¡¨ç°": ["è¡¨ç°B"],
        "å…·ä½“é”™è¯¯ç‚¹": "...",
        "é”™è¯¯ç»´åº¦å¯¼è‡´é”™è¯¯ç‚¹å‡ºç°åŸå› ": "...",
        "main": 0,
        "index": 1
    },
    {
        "é”™è¯¯ç»´åº¦": ["ç­–ç•¥æ€§"],
        "å…¸å‹è¡¨ç°": ["è¡¨ç°C"],
        "å…·ä½“é”™è¯¯ç‚¹": "...",
        "é”™è¯¯ç»´åº¦å¯¼è‡´é”™è¯¯ç‚¹å‡ºç°åŸå› ": "...",
        "main": 1,
        "index": 2
    },
    {
        "é”™è¯¯ç»´åº¦": ["æ‰§è¡Œæ€§"],
        "å…¸å‹è¡¨ç°": ["è¡¨ç°D"],
        "å…·ä½“é”™è¯¯ç‚¹": "...",
        "é”™è¯¯ç»´åº¦å¯¼è‡´é”™è¯¯ç‚¹å‡ºç°åŸå› ": "...",
        "main": 0,
        "index": 3
    },
    {
        "é”™è¯¯ç»´åº¦": ["ç»´åº¦1", "ç»´åº¦2"],
        "å…¸å‹è¡¨ç°": ["è¡¨ç°E", "è¡¨ç°F"],
        "å…·ä½“é”™è¯¯ç‚¹": "...",
        "é”™è¯¯ç»´åº¦å¯¼è‡´é”™è¯¯ç‚¹å‡ºç°åŸå› ": "...",
        "main": 0,
        "index": 4
    },
    {
        "é”™è¯¯ç»´åº¦": ["ç»´åº¦3", "ç»´åº¦4"],
        "å…¸å‹è¡¨ç°": ["è¡¨ç°G", "è¡¨ç°H"],
        "å…·ä½“é”™è¯¯ç‚¹": "...",
        "é”™è¯¯ç»´åº¦å¯¼è‡´é”™è¯¯ç‚¹å‡ºç°åŸå› ": "...",
        "main": 0,
        "index": 5
    }
]
    """
    results = []
    with ThreadPoolExecutor(max_workers=15) as executor:
        future_to_question = {
            executor.submit(process_question, q, system_prompt, error_dir, api_key, model): q
            for q in test_questions
        }
        for future in tqdm(as_completed(future_to_question), total=len(test_questions), desc="ç”Ÿæˆé”™è¯¯æ•°æ®ä¸­"):
            try:
                result = future.result()
                results.append(result)
            except Exception as exc:
                results.append(f"âŒ çº¿ç¨‹å¼‚å¸¸ï¼š{exc}")

def generate_student():
    GENERATION_COUNT = 6 
    prompt_text = """
    è§’è‰²: ä½ æ˜¯ä¸€ä½èµ„æ·±çš„æ•™è‚²å¿ƒç†å­¦ä¸“å®¶å’ŒAIæç¤ºå·¥ç¨‹å¸ˆã€‚

    èƒŒæ™¯: æˆ‘æ­£åœ¨è¿›è¡Œä¸€é¡¹æ•™è‚²ç ”ç©¶é¡¹ç›®ï¼Œéœ€è¦æ„å»ºä¸€ç³»åˆ—é«˜åº¦é€¼çœŸçš„è™šæ‹Ÿå­¦ç”Ÿæ¨¡å‹ã€‚è¿™äº›æ¨¡å‹å°†ç”¨äºæ¨¡æ‹Ÿå­¦ç”Ÿåœ¨è§£ç­”é—®é¢˜ï¼ˆå°¤å…¶æ˜¯æ•°ç†é€»è¾‘é—®é¢˜ï¼‰æ—¶çš„çœŸå®äº’åŠ¨åœºæ™¯ã€‚æˆ‘å·²ç»é€šè¿‡è¯¾å ‚è§‚å¯Ÿï¼Œæ€»ç»“å‡ºäº†å­¦ç”Ÿåœ¨è¯­è¨€è¡¨è¾¾å’Œè§£é¢˜å¿ƒæ€ä¸Šçš„å‡ ä¸ªæ ¸å¿ƒç»´åº¦ç±»å‹ã€‚

    ä»»åŠ¡: è¯·ä½ æ ¹æ®æˆ‘æä¾›çš„ã€å­¦ç”Ÿè¡¨è¾¾ç»´åº¦ã€‘å’Œã€å­¦ç”Ÿæƒ…æ„Ÿç»´åº¦ã€‘çš„è¡¨æ ¼ï¼Œå°†è¿™ä¸¤è€…è¿›è¡Œæœ‰é€»è¾‘çš„ç»„åˆï¼Œç”Ÿæˆ6ä¸ªå…·æœ‰é²œæ˜ä¸ªæ€§ç‰¹å¾çš„å­¦ç”Ÿç”»åƒã€‚

    æ ¸å¿ƒè¦æ±‚:
    1.  **ä¸¥æ ¼çš„è¾“å‡ºæ ¼å¼**: æœ€ç»ˆç»“æœå¿…é¡»æ˜¯ä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡æ•°ç»„ï¼ˆa list of JSON objectsï¼‰ã€‚æ¯ä¸ªJSONå¯¹è±¡ä»£è¡¨ä¸€ä¸ªå­¦ç”Ÿï¼Œä¸”ä¸åŒ…å«ä»»ä½•é¢å¤–çš„è§£é‡Šæ€§æ–‡æœ¬æˆ–æ³¨é‡Šã€‚
    2.  **å­—æ®µç²¾ç¡®åŒ¹é…**: æ¯ä¸ªJSONå¯¹è±¡å¿…é¡»åŒ…å«ä»¥ä¸‹ä¸‰ä¸ªå­—æ®µï¼š
        * "å­¦ç”Ÿè¡¨è¾¾ç»´åº¦": ä»æˆ‘æä¾›çš„"å­¦ç”Ÿè¡¨è¾¾ç»´åº¦"ä¸­é€‰å–çš„ç±»å‹ã€‚
        * "å­¦ç”Ÿæƒ…æ„Ÿç»´åº¦": ä»æˆ‘æä¾›çš„"å­¦ç”Ÿæƒ…æ„Ÿç»´åº¦"ä¸­é€‰å–çš„ç±»å‹ã€‚
        * "å­¦ç”Ÿæè¿°": ä¸€æ®µå…·ä½“ã€ç”ŸåŠ¨çš„å­¦ç”Ÿåˆ»ç”»æè¿°ã€‚è¿™æ®µæè¿°éœ€è¦æ·±åº¦èåˆæ‰€é€‰çš„è¡¨è¾¾ä¸æƒ…æ„Ÿç±»å‹ï¼Œæç»˜å‡ºè¯¥å­¦ç”Ÿåœ¨**è§£ç­”ä¸€ä¸ªå…·ä½“é—®é¢˜æ—¶ï¼ˆä¸ç»™å‡ºå…·ä½“ä¾‹å­éœ€è¦ä½ æ€»ç»“ä¸€ä¸ªé€šç”¨çš„å¿ƒç†è¿‡ç¨‹ï¼‰**çš„å®Œæ•´æ€è€ƒè¿‡ç¨‹ã€è¯­è¨€é£æ ¼å’Œå¿ƒç†çŠ¶æ€ã€‚æè¿°éœ€åŒ…å«ä¸°å¯Œçš„ç»†èŠ‚ï¼Œå±•ç°å…¶å†…åœ¨çš„é€»è¾‘æ–­ç‚¹æˆ–æƒ…æ„ŸæŒ£æ‰ã€‚

    è¾“å…¥æ•°æ®:

    **è¡¨1: å­¦ç”Ÿè¡¨è¾¾ç»´åº¦ (Student Expression Dimensions)**
    | è¡¨è¾¾ç±»å‹ (Type) | æ ¸å¿ƒç‰¹å¾ (Core Feature) |
    | :--- | :--- |
    | **è¯­è¨€æ¨¡ç³Šä¸æŒ‡ä»£ä¸æ¸…** | ä½¿ç”¨"è¿™ä¸ª"ã€"é‚£ä¸ª"ç­‰æ¨¡ç³Šè¯æ±‡ï¼Œè§£é¢˜æ­¥éª¤éš¾ä»¥è¿½è¸ªã€‚ä¾‹å¦‚ï¼š"å…ˆæŠŠé‚£ä¸ª...æ‹¬å·å¤–é¢çš„2å¼„è¿›å»...å°±å˜æˆ2x...ç„¶åé‚£ä¸ª6...ç­‰äº10ã€‚" |
    | **è·³è·ƒå¼é™ˆè¿°** | çœç•¥å…³é”®çš„ä¸­é—´æ€ç»´æ­¥éª¤ï¼Œç›´æ¥è·³åˆ°ç»“è®ºã€‚ä¾‹å¦‚ï¼š"å°±æ˜¯...2xç­‰äº4ï¼Œæ‰€ä»¥xç­‰äº2ã€‚"ï¼ˆæœªè§£é‡Šå¦‚ä½•ä» `2x+6=10` åˆ° `2x=4`ï¼‰ |
    | **è‡ªæˆ‘ä¿®æ­£ä¸ä¸ç¡®å®šæ€§** | è¡¨è¾¾çŠ¹è±«ã€è‡ªæˆ‘æ€€ç–‘å’Œä¿®æ­£ã€‚ä¾‹å¦‚ï¼š"å…ˆæŠŠ2ä¹˜è¿›å»ï¼Œå¾—åˆ°2xåŠ ...åŠ 6ï¼Ÿå¯¹ï¼Œæ˜¯åŠ 6ã€‚ç„¶å...æŠŠ6ç§»åˆ°å³è¾¹ï¼Ÿå¥½åƒæ˜¯...å‡6ï¼Ÿæ‰€ä»¥æ˜¯10å‡6ï¼Œå¾—4ã€‚" |
    | **"é»‘è¯"ä¸éæ­£å¼è¯­è¨€** | ä½¿ç”¨éæ ‡å‡†æœ¯è¯­ã€‚ä¾‹å¦‚ï¼š"è€å¸ˆæ•™çš„æ˜¯'ç§»é¡¹å˜å·'ï¼Œæ‰€ä»¥æŠŠ6'æ‰”'è¿‡å»å°±å˜æˆå‡6äº†ã€‚" |
    | **å†—ä½™ä¸é‡å¤** | åå¤é™ˆè¿°åŒä¸€ä¸ªæ­¥éª¤æˆ–æƒ³æ³•ã€‚ä¾‹å¦‚ï¼š"å°±æ˜¯2ä¹˜ä»¥xï¼Œç„¶å2å†ä¹˜ä»¥3ã€‚å¯¹ï¼Œå°±æ˜¯2ä¹˜ä»¥xï¼Œç„¶å2ä¹˜ä»¥3ã€‚ç„¶ååŠ èµ·æ¥ã€‚" |
    | **å…·è±¡åŒ–æè¿°** | ä¾èµ–äºå…·ä½“äº‹ç‰©æˆ–åŠ¨ä½œæ¥æè¿°æŠ½è±¡è¿‡ç¨‹ã€‚ä¾‹å¦‚ï¼š"æˆ‘æœ‰2ä¸ªè‹¹æœï¼Œåˆæ‹¿æ¥3ä¸ªè‹¹æœï¼Œæˆ‘æ•°ä¸€ä¸‹...æ˜¯5ä¸ªã€‚"ï¼ˆç”¨æ­¤é£æ ¼æè¿°ä»£æ•°ï¼‰ |

    **è¡¨2: å­¦ç”Ÿæƒ…æ„Ÿ/å¿ƒæ€ç»´åº¦ (Student Emotional/Mental Dimensions)**
    | æƒ…æ„Ÿç±»å‹ (Type) | æ ¸å¿ƒç‰¹å¾ (Core Feature) |
    | :--- | :--- |
    | **è¿‡åº¦è‡ªä¿¡çš„"ä¸“å®¶"** | è®¤ä¸ºè‡ªå·±å®Œå…¨æŒæ¡ï¼Œä½†å®åˆ™å­˜åœ¨æ¦‚å¿µé”™è¯¯ã€‚è¡¨è¾¾æµåˆ©è‡ªä¿¡ï¼Œä¸å®¹ç½®ç–‘ã€‚ |
    | **ç„¦è™‘ä¸å®‰çš„"å°ç™½"** | ç¼ºä¹ä¿¡å¿ƒï¼Œå®³æ€•çŠ¯é”™ã€‚è¡¨è¾¾æ—¶å……æ»¡"å¯èƒ½"ã€"ä¹Ÿè®¸"ã€"æˆ‘ä¸çŸ¥é“å¯¹ä¸å¯¹"ç­‰ä¸ç¡®å®šè¯æ±‡ã€‚ |
    | **"æˆ‘å¿˜äº†"çš„ç”©æ‰‹æŒæŸœ** | ä»¥"å¿˜äº†"ã€"è€å¸ˆå°±æ˜¯è¿™ä¹ˆæ•™çš„"æ¥å›é¿å¯¹åŸç†çš„æ·±å±‚è§£é‡Šã€‚ |
    | **å›ºæ‰§çš„"ä¸€æ¡è·¯èµ°åˆ°é»‘"** | åšä¿¡è‡ªå·±çš„ï¼ˆé”™è¯¯ï¼‰æ–¹æ³•æ˜¯å”¯ä¸€è§£æ³•ï¼Œå³ä½¿é‡åˆ°å›°éš¾ä¹Ÿä¸æ„¿å°è¯•å…¶ä»–è·¯å¾„ã€‚ |
    | **å¯»æ±‚å¿«é€Ÿç­”æ¡ˆçš„"åŠŸåˆ©è€…"** | å¯¹è¿‡ç¨‹ä¸æ„Ÿå…´è¶£ï¼Œåªæƒ³çŸ¥é“æœ€ç»ˆç­”æ¡ˆå’Œè€ƒè¯•è€ƒæ³•ã€‚ |
    **ã€JSON è¾“å‡ºæ ¼å¼ã€‘**
    ```json
    {
    "1": {
        "å­¦ç”Ÿè¡¨è¾¾ç»´åº¦": ["ç»´åº¦1", "ç»´åº¦2"],
        "å­¦ç”Ÿæƒ…æ„Ÿç»´åº¦": ["ç»´åº¦A", "ç»´åº¦B"],
        "å­¦ç”Ÿæè¿°": "ä¸€æ®µå…·ä½“ã€ç”ŸåŠ¨çš„å­¦ç”Ÿåˆ»ç”»æè¿°ã€‚",    },
    "2": {
        "å­¦ç”Ÿè¡¨è¾¾ç»´åº¦": ["ç»´åº¦3"],
        "å­¦ç”Ÿæƒ…æ„Ÿç»´åº¦": ["ç»´åº¦C"],
        "å­¦ç”Ÿæè¿°": "ä¸€æ®µå…·ä½“ã€ç”ŸåŠ¨çš„å­¦ç”Ÿåˆ»ç”»æè¿°ã€‚",
    },
    ...
    }
    """
    user_prompt = """ """
    api_key = "sk-fKhKDrWY5AxX5FPeZO95LPieEqgt6yL6IpdlBZFI0AEOx2Cd"  
    model = "gemini-2.5-pro-exp-03-25"

    result = call_api(prompt_text, user_prompt, api_key=api_key, model=model, max_tokens=9999)

    if result:
        logging.info("å¼€å§‹å°†æ¨¡å‹è¾“å‡ºå†™å…¥ TXT æ–‡ä»¶ã€‚")

        try:
            result_cleaned = result.strip()
            if result_cleaned.startswith("```json"):
                result_cleaned = result_cleaned[7:].lstrip()
            if result_cleaned.endswith("```"):
                result_cleaned = result_cleaned[:-3].rstrip()

            output_path = Path("student_output.txt")
            with output_path.open("w", encoding="utf-8") as f:
                f.write(result_cleaned)

            logging.info(f"å†™å…¥æˆåŠŸï¼Œæ–‡ä»¶è·¯å¾„: {output_path.resolve()}")
        except Exception as e:
            logging.error(f"å†™å…¥ TXT æ–‡ä»¶å¤±è´¥ï¼š{e}")
            logging.error(f"åŸå§‹æ¨¡å‹è¾“å‡ºï¼š\n{result}")
    else:
        logging.error("API è¿”å›ä¸ºç©ºï¼Œæœªèƒ½ç”Ÿæˆé”™è¯¯æ•°æ®ã€‚")

def extract_and_remove_think(text):
    if not isinstance(text, str) or not text:
        return [], ""
    thinks = re.findall(r'<think>(.*?)</think>', text, re.DOTALL)
    text_wo_think = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
    return thinks, text_wo_think.strip()

from openai import OpenAI # Use the new OpenAI class

# æ­¤å‡½æ•°å·²æ›´æ–°ï¼Œä»…ä¿®æ”¹äº†æ•™å¸ˆæ¨¡å‹çš„APIè°ƒç”¨æ–¹å¼
def run_one_dialogue(question, error, dialogue_id, all_question_map=None, output_dir=None, teacher_model="gpt-4o-mini", teacher_api_key=None):
    rounds_max = 20
    teacher_system_prompt = '''
Please act as a professional and dedicated teacher. You will engage in a dialogue with a student who is asking you questions. Your sole objective is to use Socratic questioning to precisely identify the student's error in solving the given problem.
Your task is only to identify the mistake, not to guide the student toward the correct answer.
Your constraints are as follows:
**Do not guide, hint, or ask the student to reply with "zzc." Do not directly ask for the student's complete incorrect problem-solving steps.
**Only when you are 100% certain of the student's mistake can you point it out with a clear and concise statement. After diagnosing and stating the error, wait for the student's final response. If the student does not reply with "zzc," try a different line of questioning or continue the dialogue, do not repeatedly confirm the same error or ask the same question.
**The prompt history contains past dialogues. Strictly avoid mimicking the student's tone or restating their answers for analysis. You must always maintain the demeanor and tone of a professional teacher.
**Your reply to student must not exceed 80 tokens. Please keep your questions and answers concise.
'''
    student_system_prompt =f'''
You are a student who made a mistake in solving a problem, with the error detailed in ã€Specific Error JSONã€‘. You will engage in direct dialogue with the teacher, responding to their questions about your mistake.
ã€Specific Error JSONã€‘:
{json.dumps(error, ensure_ascii=False, indent=4)}
Your constraints are as follows:
**Your initial response must begin with your incorrect answer.
**Do not reveal the entire incorrect solving process at once. Only provide the information necessary to answer the teacher's current question.
**If the teacher points out your mistake in a statement and this accusation exactly matches the reason in ã€Specific Error JSONã€‘, you must respond with the exact string "zzc". If the teacher states an error that does not match your actual mistake, you should deny it and wait for the teacher's next question. If the teacher asks you a question (i.e., a sentence ending with a question mark), you must answer it directly. Never respond with "zzc" when answering a question, regardless of whether you think it reveals your mistake.
**Your reply to teacher must not exceed 80 tokens. Please keep your questions and answers concise.
'''
    prompt_history = ""
    round_used = 0
    usage_list = []
    for round_num in range(rounds_max):
        log_prefix = f"[dialogue_id={dialogue_id}][round={round_num+1}]"
        logging.info(f"{log_prefix} === ç¬¬ {round_num + 1} è½®å¯¹è¯ ===")
        round_used = round_num + 1
        current_prompt = f"""
        math questionï¼š{question['question']}
        max dialogue roundsï¼š{rounds_max}
        prompt historyï¼š
        {prompt_history}
        """
        # å­¦ç”Ÿå›å¤ï¼ˆllm_inf.pyè°ƒç”¨ï¼‰- This part remains unchanged
        from llm_inf import LLMHandler_inf
        student_handler = LLMHandler_inf(api_key="sk-gOuCvUoxCi7qZ5ORL8aZkKRQDe49gi3t2rDjjl4OiViCi9VW", model="gpt-4.1")
        student_completion = student_handler.get_completion([
            {"role": "system", "content": student_system_prompt},
            {"role": "user", "content": current_prompt}
        ], temperature=1.0, max_tokens=2048)
        student_response = student_completion
        if not isinstance(student_response, str) or not student_response:
            logging.warning(f"{log_prefix} å­¦ç”Ÿå›å¤ä¸ºç©ºæˆ–éå­—ç¬¦ä¸²ï¼Œè·³è¿‡æœ¬è½®ã€‚student_response={student_response}")
            student_thinks, student_response_wo_think = [], ""
        else:
            # Assuming extract_and_remove_think is defined elsewhere
            student_thinks, student_response_wo_think = extract_and_remove_think(student_response)
        for t in student_thinks:
            logging.info(f"{log_prefix} å­¦ç”Ÿ<think>: {t}")
        logging.info(f"{log_prefix} å­¦ç”Ÿï¼š{student_response_wo_think}")
        prompt_history += f"\nStudentï¼š{student_response_wo_think}"
        if "zzc" in student_response_wo_think:
            logging.info(f"{log_prefix} å­¦ç”Ÿç»ˆæ­¢å¯¹è¯")
            break
            
        # ==================== MODIFICATION START: æ•™å¸ˆAPIè°ƒç”¨ä¿®æ”¹ä¸ºQwen ====================
        
        # 1. åˆ›å»ºæŒ‡å‘é˜¿é‡Œäº‘é€šä¹‰åƒé—®å…¼å®¹æ¨¡å¼ç«¯ç‚¹çš„å®¢æˆ·ç«¯
        client = OpenAI(
            api_key=teacher_api_key,
            # MODIFIED: æ›´æ–°ä¸ºé€šä¹‰åƒé—®çš„base_url
            base_url="https://www.dmxapi.com/v1",
        )
        
        teacher_messages = [
            {"role": "system", "content": teacher_system_prompt},
            {"role": "user", "content": current_prompt + f"\nStudentï¼š{student_response_wo_think}"},
        ]

        # å‡†å¤‡APIè°ƒç”¨å‚æ•°
        api_params = {
            "model": teacher_model,
            "messages": teacher_messages,
            "temperature": 1.0,
        }

        # MODIFIED: æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼Œä¸ºqwen2/qwen3æ¨¡å‹æ·»åŠ extra_bodyå‚æ•°ä»¥å…³é—­æ€è€ƒè¿‡ç¨‹
        # è¿™å¯ä»¥é˜²æ­¢åœ¨éæµå¼è¾“å‡ºæ—¶æŠ¥é”™
        if "qwen2" in teacher_model or "qwen3" in teacher_model:
            api_params["extra_body"] = {"enable_thinking": False}
        
        try:
            # 2. ä½¿ç”¨æ›´æ–°åçš„å‚æ•°è°ƒç”¨API
            response = client.chat.completions.create(**api_params)
            
            teacher_response = response.choices[0].message.content
            
            # 3. è®°å½•usageï¼Œé€šä¹‰åƒé—®å…¼å®¹æ¨¡å¼è¿”å›çš„usageä¸OpenAIæ ¼å¼ç›¸åŒï¼Œæ— éœ€ä¿®æ”¹
            if response.usage:
                usage_list.append(response.usage.model_dump())
            else:
                usage_list.append({})

        except Exception as e:
            teacher_response = f"[Qwen APIè¯·æ±‚å¼‚å¸¸: {e}]"
            logging.error(f"Qwen APIè¯·æ±‚å¼‚å¸¸: {e}")
            usage_list.append({})
        # ===================== MODIFICATION END =====================

        # Assuming extract_and_remove_think is defined elsewhere
        teacher_thinks, teacher_response_wo_think = extract_and_remove_think(teacher_response)
        for t in teacher_thinks:
            logging.info(f"{log_prefix} æ•™å¸ˆ<think>: {t}")
        logging.info(f"{log_prefix} æ•™å¸ˆï¼š{teacher_response_wo_think}")
        prompt_history += f"\nTeacherï¼š{teacher_response_wo_think}"

    correct_answer = None
    question_index = None
    if all_question_map is not None and hasattr(error, 'get'):
        question_index = error.get("question_index")
        qobj = all_question_map.get(question_index)
        if qobj:
            correct_answer = qobj.get("answer")
            
    output_data = {
        "dialogue_id": dialogue_id,
        "question": question['question'],
        "correct_answer": correct_answer,
        "question_index": question_index,
        "error": error,
        "student_system_prompt": student_system_prompt,
        "teacher_system_prompt": teacher_system_prompt,
        "history_prompt": prompt_history,
        "rounds_used": round_used,
        "rounds_max": rounds_max,
        "end_time": datetime.now().isoformat(),
        "usage_list": usage_list,
        "teacher_model": teacher_model
    }
    if output_dir is None:
        dialogue_dir = Path(__file__).parent / f'{teacher_model}_results'
    else:
        dialogue_dir = Path(output_dir)
    dialogue_dir.mkdir(exist_ok=True)
    out_path = dialogue_dir / f"dialogue_{question_index}_result.json"
    with open(out_path, "a", encoding="utf-8") as f:
        json.dump(output_data, f, ensure_ascii=False, indent=4)
    logging.info(f"[dialogue_id={dialogue_id}] ç¬¬ {dialogue_id} ä¸ªå¯¹è¯è¾“å‡ºå®Œæ¯•ï¼Œå·²ä¿å­˜åˆ° {out_path}")
    return dialogue_id, usage_list, round_used

def run_dialogue_multithread(error_output, all_question_map, output_root_dir, teacher_model, teacher_api_key):
    dialogue_id = 0
    os.makedirs(output_root_dir, exist_ok=True)
    usage_all = []
    rounds_per_dialogue = []  # æ–°å¢åˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨æ¯æ¬¡å¯¹è¯çš„è½®æ•°

    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        futures = []
        for i, error in enumerate(error_output):
            question_index = error.get("question_index")
            question = all_question_map.get(question_index, "ï¼ˆæœªæ‰¾åˆ°åŸå§‹é¢˜ç›®ï¼‰")
            dialogue_id += 1
            student_dir = os.path.join(output_root_dir, f"student{i}")
            os.makedirs(student_dir, exist_ok=True)
            futures.append(executor.submit(
                run_one_dialogue, question, error, dialogue_id, all_question_map, student_dir, teacher_model, teacher_api_key
            ))

        for f in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc="å¯¹è¯æ¨¡æ‹Ÿè¿›åº¦"):
            try:
                # ä»è¿”å›ç»“æœä¸­æ‹†è§£å‡ºæ–°çš„ rounds_used å€¼
                result_id, usage_list, rounds_used = f.result()
                usage_all.extend(usage_list)
                rounds_per_dialogue.append(rounds_used)  # å°†è½®æ•°æ·»åŠ åˆ°æ–°åˆ—è¡¨ä¸­
                tqdm.write(f"å¯¹è¯ {result_id} å·²å®Œæˆ")
            except Exception as e:
                tqdm.write(f"ä¸€ä¸ªå¯¹è¯çº¿ç¨‹å¤±è´¥: {e}")

    # --- æ–°çš„ç»Ÿè®¡è®¡ç®—éƒ¨åˆ†ï¼ˆè®¡ç®—å¹³å‡å€¼ï¼‰ ---

    # è·å–ç”¨äºè®¡ç®—å¹³å‡å€¼çš„åˆ†æ¯
    num_successful_dialogues = len(rounds_per_dialogue)
    num_total_teacher_turns = len(usage_all)

    # è®¡ç®—Tokenæ€»æ•°ï¼ˆä¸ä¹‹å‰ç›¸åŒï¼‰
    total_prompt_tokens = sum(u.get('prompt_tokens', 0) for u in usage_all if isinstance(u, dict))
    total_completion_tokens = sum(u.get('completion_tokens', 0) for u in usage_all if isinstance(u, dict))

    # è®¡ç®—å¹³å‡å€¼ï¼Œå¹¶å¤„ç†å¯èƒ½å‡ºç°çš„é™¤ä»¥é›¶é”™è¯¯
    avg_rounds_per_dialogue = sum(rounds_per_dialogue) / num_successful_dialogues if num_successful_dialogues > 0 else 0
    avg_prompt_tokens_per_turn = total_prompt_tokens / num_total_teacher_turns if num_total_teacher_turns > 0 else 0
    avg_completion_tokens_per_turn = total_completion_tokens / num_total_teacher_turns if num_total_teacher_turns > 0 else 0

    # åˆ›å»ºåŒ…å«å¹³å‡å€¼çš„æ–°ç»Ÿè®¡å¯¹è±¡
    stat = {
        "model_name": teacher_model,
        "num_successful_dialogues": num_successful_dialogues,
        "avg_rounds_per_dialogue": f"{avg_rounds_per_dialogue:.2f}",
        "avg_prompt_tokens_per_turn": f"{avg_prompt_tokens_per_turn:.2f}",
        "avg_completion_tokens_per_turn": f"{avg_completion_tokens_per_turn:.2f}",
        "total_completion_tokens": total_completion_tokens # åŒæ—¶ä¿ç•™æ€»æ•°ä»¥ä¾›å‚è€ƒ
    }

    with open(os.path.join(output_root_dir, "stat.json"), "w", encoding="utf-8") as f:
        json.dump(stat, f, ensure_ascii=False, indent=4)
    
    print("--- ç»Ÿè®¡æ‘˜è¦ ---")
    print(json.dumps(stat, indent=4, ensure_ascii=False))
# This function remains unchanged
def load_questions_map(filepath):
    with open(filepath, "r", encoding="utf-8") as f:
        all_questions = json.load(f)
    return {q["index"]: q for q in all_questions}

# This section remains unchanged
if __name__ == "__main__":
    # Assuming extract_and_remove_think is defined somewhere in your actual file
    def extract_and_remove_think(text):
        # A dummy implementation for the code to be runnable
        return [], text

    logging.basicConfig(level=logging.INFO) # Basic config for logging
    
    start_time_str = datetime.now().strftime("dialogue_%Y%m%d_%H%M%S")
    all_models = [
        "gemini-2.5-pro-exp-03-25"
    ]
    teacher_api_key = "sk-fKhKDrWY5AxX5FPeZO95LPieEqgt6yL6IpdlBZFI0AEOx2Cd"  # ä½ çš„DMXAPI key
    
    # Create dummy files for testing
    if not os.path.exists("a_new_questions_10.json"):
        with open("a_new_questions_10.json", "w", encoding="utf-8") as f:
            json.dump([{"index": 1, "question": "What is 2+2?", "answer": "4"}], f)
    if not os.path.exists("a_errors_10.json"):
        with open("a_errors_10.json", "w", encoding="utf-8") as f:
            json.dump([{"question_index": 1, "error_reason": "I thought 2+2=5"}], f)

    question_map = load_questions_map("a_new_questions_10.json")
    with open("a_errors_10.json", "r", encoding="utf-8") as f:
        error_output = json.load(f)
    
    all_model_stats = []
    result_root_dir = os.path.join(os.path.dirname(__file__), "result-v4")
    for model in all_models:
        print(f"--- Running simulation for teacher model: {model} ---")
        output_root_dir = os.path.join(result_root_dir, f"{model}_results")
        run_dialogue_multithread(error_output, question_map, output_root_dir, model, teacher_api_key)